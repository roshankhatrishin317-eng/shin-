{"type":"queue-operation","operation":"enqueue","timestamp":"2025-11-08T12:45:48.163Z","content":[{"type":"text","text":"<ide_opened_file>The user opened the file /teamspace/studios/this_studio/.claude/settings.json in the IDE. This may or may not be related to the current task.</ide_opened_file>"},{"type":"text","text":"restart lite llm server"}],"sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad"}
{"type":"queue-operation","operation":"dequeue","timestamp":"2025-11-08T12:45:48.163Z","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad"}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"type":"text","text":"<ide_opened_file>The user opened the file /teamspace/studios/this_studio/.claude/settings.json in the IDE. This may or may not be related to the current task.</ide_opened_file>"},{"type":"text","text":"restart lite llm server"}]},"uuid":"28bfab71-c6bd-4a00-afd7-544203e464f5","timestamp":"2025-11-08T12:45:48.172Z"}
{"parentUuid":"28bfab71-c6bd-4a00-afd7-544203e464f5","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204549a63c87496d5f439b","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"I'll help you restart the LiteLLM server. Let me first check if there's an existing LiteLLM server running and understand the current setup.\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3309,"output_tokens":69,"cache_read_input_tokens":10330}},"type":"assistant","uuid":"5dc6ae2e-2e90-4f21-9931-73c954be166f","timestamp":"2025-11-08T12:45:50.522Z"}
{"parentUuid":"5dc6ae2e-2e90-4f21-9931-73c954be166f","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204549a63c87496d5f439b","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_e0zdwaazsz","name":"Bash","input":{"command":"ps aux | grep litellm","description":"Check for running LiteLLM processes"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3309,"output_tokens":69,"cache_read_input_tokens":10330}},"type":"assistant","uuid":"41f24a8b-9a9d-4e40-8b6b-f85ad0b80086","timestamp":"2025-11-08T12:45:50.529Z"}
{"parentUuid":"41f24a8b-9a9d-4e40-8b6b-f85ad0b80086","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_e0zdwaazsz","type":"tool_result","content":"roshans+    9949  0.4  1.8 850712 302552 pts/9   Sl+  11:18   0:25 /home/zeus/miniconda3/envs/cloudspace/bin/python3.12 /home/zeus/miniconda3/envs/cloudspace/bin/litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug\nroshans+   34184 15.5  0.0  12268  8064 ?        Ss   12:45   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'ps aux < /dev/null | grep litellm' && pwd -P >| /tmp/claude-6da5-cwd\nroshans+   34701  0.0  0.0   7084  2176 ?        S    12:45   0:00 grep litellm","is_error":false}]},"uuid":"35d42054-a174-4f6e-b428-0c103b4ec2ce","timestamp":"2025-11-08T12:45:51.068Z","toolUseResult":{"stdout":"roshans+    9949  0.4  1.8 850712 302552 pts/9   Sl+  11:18   0:25 /home/zeus/miniconda3/envs/cloudspace/bin/python3.12 /home/zeus/miniconda3/envs/cloudspace/bin/litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug\nroshans+   34184 15.5  0.0  12268  8064 ?        Ss   12:45   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'ps aux < /dev/null | grep litellm' && pwd -P >| /tmp/claude-6da5-cwd\nroshans+   34701  0.0  0.0   7084  2176 ?        S    12:45   0:00 grep litellm","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"35d42054-a174-4f6e-b428-0c103b4ec2ce","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204551877a7f8564f547f8","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"I can see there's a LiteLLM server running on PID 9949. Let me stop it first and then restart it.\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":271,"output_tokens":64,"cache_read_input_tokens":13707}},"type":"assistant","uuid":"9c612735-cf78-457b-b345-772f86912086","timestamp":"2025-11-08T12:45:52.707Z"}
{"parentUuid":"9c612735-cf78-457b-b345-772f86912086","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204551877a7f8564f547f8","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_lfm5j2xlvs","name":"Bash","input":{"command":"kill 9949","description":"Stop the running LiteLLM server"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":271,"output_tokens":64,"cache_read_input_tokens":13707}},"type":"assistant","uuid":"3e6af717-5e6c-42f7-aae1-64b353fda1a8","timestamp":"2025-11-08T12:45:52.713Z"}
{"parentUuid":"3e6af717-5e6c-42f7-aae1-64b353fda1a8","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_lfm5j2xlvs","type":"tool_result","content":"","is_error":false}]},"uuid":"ddf73b2a-8d2c-4e3c-aa0d-59f8d001fdcb","timestamp":"2025-11-08T12:45:59.039Z","toolUseResult":{"stdout":"","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"ddf73b2a-8d2c-4e3c-aa0d-59f8d001fdcb","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_202511082046000362e67849d94de9","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"Now let me verify the server has stopped and check the config file:\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"output_tokens":52,"cache_read_input_tokens":14041}},"type":"assistant","uuid":"cea5befa-a2af-42f9-8b20-da298a1c948e","timestamp":"2025-11-08T12:46:02.540Z"}
{"parentUuid":"cea5befa-a2af-42f9-8b20-da298a1c948e","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_202511082046000362e67849d94de9","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_kq2teqax6i","name":"Bash","input":{"command":"ps aux | grep litellm","description":"Verify LiteLLM server is stopped"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":6,"output_tokens":52,"cache_read_input_tokens":14041}},"type":"assistant","uuid":"f5e2753f-9b3f-446b-add4-29be5cf1542b","timestamp":"2025-11-08T12:46:02.547Z"}
{"parentUuid":"f5e2753f-9b3f-446b-add4-29be5cf1542b","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_kq2teqax6i","type":"tool_result","content":"roshans+   35294 17.3  0.0  12268  7936 ?        Ss   12:46   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'ps aux < /dev/null | grep litellm' && pwd -P >| /tmp/claude-fed3-cwd\nroshans+   35811  0.0  0.0   7084  2176 ?        S    12:46   0:00 grep litellm","is_error":false}]},"uuid":"8337fab3-dd20-4d77-aa5e-81aa8188c4ce","timestamp":"2025-11-08T12:46:03.024Z","toolUseResult":{"stdout":"roshans+   35294 17.3  0.0  12268  7936 ?        Ss   12:46   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'ps aux < /dev/null | grep litellm' && pwd -P >| /tmp/claude-fed3-cwd\nroshans+   35811  0.0  0.0   7084  2176 ?        S    12:46   0:00 grep litellm","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"8337fab3-dd20-4d77-aa5e-81aa8188c4ce","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_2025110820460432dfeef8d8c747ba","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"Good, the server has stopped. Now let me check the config file to see how it was configured:\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"7e6a916f-c1b2-4c25-ba23-40c1d5bc1288","timestamp":"2025-11-08T12:46:06.874Z"}
{"parentUuid":"7e6a916f-c1b2-4c25-ba23-40c1d5bc1288","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_2025110820460432dfeef8d8c747ba","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_ypdzmvg2m9","name":"Read","input":{"file_path":"/teamspace/studios/this_studio/config.yaml"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"ccd2b3c2-8ae5-48fb-9ba5-223cbe19089f","timestamp":"2025-11-08T12:46:06.927Z"}
{"parentUuid":"ccd2b3c2-8ae5-48fb-9ba5-223cbe19089f","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_ypdzmvg2m9","type":"tool_result","content":"     1→model_list:\n     2→  # Azure OpenAI - Reference: https://docs.litellm.ai/docs/providers/azure\n     3→  # Uncomment and configure if needed\n     4→  # - model_name: azure-gpt-4\n     5→  #   litellm_params:\n     6→  #     model: azure/<your-deployment-name>\n     7→  #     api_base: os.environ/AZURE_API_BASE\n     8→  #     api_key: os.environ/AZURE_API_KEY\n     9→  #     api_version: \"2023-07-01-preview\"\n    10→  \n    11→  # NVIDIA NIM Models - Reference: https://docs.api.nvidia.com/nim/reference/llm-apis\n    12→  \n    13→  # NVIDIA Llama Models\n    14→  - model_name: shin-llama-3-1-nemotron-70b\n    15→    litellm_params:\n    16→      model: nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct\n    17→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    18→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    19→  \n    20→  - model_name: shin-llama-3-1-405b\n    21→    litellm_params:\n    22→      model: nvidia_nim/meta/llama-3.1-405b-instruct\n    23→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    24→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    25→  \n    26→  - model_name: shin-llama-3-1-70b\n    27→    litellm_params:\n    28→      model: nvidia_nim/meta/llama-3.1-70b-instruct\n    29→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    30→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    31→  \n    32→  - model_name: shin-llama-3-1-8b\n    33→    litellm_params:\n    34→      model: nvidia_nim/meta/llama-3.1-8b-instruct\n    35→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    36→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    37→  \n    38→  # NVIDIA Mistral Models\n    39→  - model_name: shin-mistral-nemo-12b\n    40→    litellm_params:\n    41→      model: nvidia_nim/mistralai/mistral-nemo-12b-instruct\n    42→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    43→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    44→  \n    45→  - model_name: shin-mistral-large-2\n    46→    litellm_params:\n    47→      model: nvidia_nim/mistralai/mistral-large-2-instruct\n    48→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    49→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    50→  \n    51→  # MoonshotAI Kimi K2 - Available via NVIDIA NIM\n    52→  - model_name: shin-moonshotai-kimi-k2\n    53→    litellm_params:\n    54→      model: nvidia_nim/moonshotai/kimi-k2-instruct-0905\n    55→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    56→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    57→  \n    58→  # Qwen Coder - Available via NVIDIA NIM\n    59→  - model_name: shin-qwen-coder-480b\n    60→    litellm_params:\n    61→      model: nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct\n    62→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    63→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    64→  \n    65→  # MiniMax M2 - Available via NVIDIA NIM\n    66→  - model_name: shin-minimax-m2\n    67→    litellm_params:\n    68→      model: nvidia_nim/minimaxai/minimax-m2\n    69→      api_key: os.environ/NVIDIA_NIM_API_KEY\n    70→      api_base: os.environ/NVIDIA_NIM_API_BASE\n    71→  \n    72→  # Example: Ollama Local Models - Reference: https://docs.litellm.ai/docs/providers/ollama\n    73→  # - model_name: ollama-llama2\n    74→  #   litellm_params:\n    75→  #     model: ollama/llama2\n    76→  #     api_base: http://localhost:11434\n    77→  \n    78→  # OpenCode.ai Provider\n    79→  - model_name: shin-4.6\n    80→    litellm_params:\n    81→      model: openai/big-pickle\n    82→      api_key: os.environ/OPENCODE_API_KEY\n    83→      api_base: https://opencode.ai/zen/v1\n    84→  \n    85→  # iFlow Provider - Multiple Models\n    86→  - model_name: otsu-qwen3-coder-plus\n    87→    litellm_params:\n    88→      model: openai/qwen3-coder-plus\n    89→      api_key: os.environ/IFLOW_API_KEY\n    90→      api_base: https://apis.iflow.cn/v1\n    91→  \n    92→  - model_name: otsu-qwen3-max\n    93→    litellm_params:\n    94→      model: openai/qwen3-max\n    95→      api_key: os.environ/IFLOW_API_KEY\n    96→      api_base: https://apis.iflow.cn/v1\n    97→  \n    98→  - model_name: otsu-qwen3-coder\n    99→    litellm_params:\n   100→      model: openai/qwen3-coder\n   101→      api_key: os.environ/IFLOW_API_KEY\n   102→      api_base: https://apis.iflow.cn/v1\n   103→  \n   104→  - model_name: otsu-kimi-k2-0905\n   105→    litellm_params:\n   106→      model: openai/kimi-k2-0905\n   107→      api_key: os.environ/IFLOW_API_KEY\n   108→      api_base: https://apis.iflow.cn/v1\n   109→  \n   110→  - model_name: otsu-glm-4.6\n   111→    litellm_params:\n   112→      model: openai/glm-4.6\n   113→      api_key: os.environ/IFLOW_API_KEY\n   114→      api_base: https://apis.iflow.cn/v1\n   115→  \n   116→  - model_name: otsu-deepseek-v3.2\n   117→    litellm_params:\n   118→      model: openai/deepseek-v3.2\n   119→      api_key: os.environ/IFLOW_API_KEY\n   120→      api_base: https://apis.iflow.cn/v1\n   121→  \n   122→  - model_name: otsu-qwen3-235b-thinking\n   123→    litellm_params:\n   124→      model: openai/qwen3-235b-a22b-thinking-2507\n   125→      api_key: os.environ/IFLOW_API_KEY\n   126→      api_base: https://apis.iflow.cn/v1\n   127→  \n   128→  # Minimax1 Provider via anthropic endpoint\n   129→  - model_name: minimax1-claude\n   130→    litellm_params:\n   131→      model: openai/claude\n   132→      api_key: os.environ/MINIMAX1_API_KEY\n   133→      api_base: https://api.minimax.io/anthropic\n   134→\n   135→  # KAT Provider - Custom endpoint\n   136→  - model_name: shin-otsu-pro\n   137→    litellm_params:\n   138→      model: openai/ep-54wdd8-1762521555111681538\n   139→      api_key: os.environ/KAT_API_KEY\n   140→      api_base: https://vanchin.streamlake.ai/api/gateway/v1/endpoints\n   141→\n   142→  - model_name: shin-otsu-plus\n   143→    litellm_params:\n   144→      model: openai/ep-w4jwca-1760471819232090405\n   145→      api_key: os.environ/KAT_API_KEY\n   146→      api_base: https://vanchin.streamlake.ai/api/gateway/v1/endpoints\n   147→\n   148→  # Novi Provider\n   149→  - model_name: qwen-kat\n   150→    litellm_params:\n   151→      model: openai/kat-coder\n   152→      api_key: sk_1K9xQu19r9Hen-vah5svLqwaqBnlyRKb60UVqx3LEgc\n   153→      api_base: https://api.novita.ai/openai/v1\n   154→\n   155→litellm_settings:\n   156→  # Enable/disable features\n   157→  drop_params: true\n   158→  set_verbose: true\n   159→  request_timeout: 600\n   160→  \n   161→  # Caching (optional)\n   162→  # cache: true\n   163→  # cache_params:\n   164→  #   type: \"redis\"\n   165→  #   host: \"localhost\"\n   166→  #   port: 6379\n   167→\n   168→general_settings:\n   169→  master_key: os.environ/LITELLM_MASTER_KEY # Authentication key for the proxy\n   170→  #database_url: \"DATABASE_URL='postgresql://neondb_owner:npg_xOAmMe3W1nkU@ep-calm-boat-a187mu3u-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require'\" # Database disabled (optional - requires prisma setup)\n   171→  \n   172→  # Uncomment to enable detailed logging\n   173→  # log_level: \"DEBUG\"\n   174→\n   175→router_settings:\n   176→  routing_strategy: simple-shuffle # Options: simple-shuffle, least-busy, usage-based-routing\n   177→  num_retries: 3\n   178→  retry_after: 2\n   179→  timeout: 600\n   180→  \n   181→  # Load balancing (if you have multiple instances of same model)\n   182→  # enable_loadbalancing: true\n   183→\n   184→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"6f5d8fad-640d-437a-b88c-377c5618851d","timestamp":"2025-11-08T12:46:07.091Z","toolUseResult":{"type":"text","file":{"filePath":"/teamspace/studios/this_studio/config.yaml","content":"model_list:\n  # Azure OpenAI - Reference: https://docs.litellm.ai/docs/providers/azure\n  # Uncomment and configure if needed\n  # - model_name: azure-gpt-4\n  #   litellm_params:\n  #     model: azure/<your-deployment-name>\n  #     api_base: os.environ/AZURE_API_BASE\n  #     api_key: os.environ/AZURE_API_KEY\n  #     api_version: \"2023-07-01-preview\"\n  \n  # NVIDIA NIM Models - Reference: https://docs.api.nvidia.com/nim/reference/llm-apis\n  \n  # NVIDIA Llama Models\n  - model_name: shin-llama-3-1-nemotron-70b\n    litellm_params:\n      model: nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  - model_name: shin-llama-3-1-405b\n    litellm_params:\n      model: nvidia_nim/meta/llama-3.1-405b-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  - model_name: shin-llama-3-1-70b\n    litellm_params:\n      model: nvidia_nim/meta/llama-3.1-70b-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  - model_name: shin-llama-3-1-8b\n    litellm_params:\n      model: nvidia_nim/meta/llama-3.1-8b-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  # NVIDIA Mistral Models\n  - model_name: shin-mistral-nemo-12b\n    litellm_params:\n      model: nvidia_nim/mistralai/mistral-nemo-12b-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  - model_name: shin-mistral-large-2\n    litellm_params:\n      model: nvidia_nim/mistralai/mistral-large-2-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  # MoonshotAI Kimi K2 - Available via NVIDIA NIM\n  - model_name: shin-moonshotai-kimi-k2\n    litellm_params:\n      model: nvidia_nim/moonshotai/kimi-k2-instruct-0905\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  # Qwen Coder - Available via NVIDIA NIM\n  - model_name: shin-qwen-coder-480b\n    litellm_params:\n      model: nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  # MiniMax M2 - Available via NVIDIA NIM\n  - model_name: shin-minimax-m2\n    litellm_params:\n      model: nvidia_nim/minimaxai/minimax-m2\n      api_key: os.environ/NVIDIA_NIM_API_KEY\n      api_base: os.environ/NVIDIA_NIM_API_BASE\n  \n  # Example: Ollama Local Models - Reference: https://docs.litellm.ai/docs/providers/ollama\n  # - model_name: ollama-llama2\n  #   litellm_params:\n  #     model: ollama/llama2\n  #     api_base: http://localhost:11434\n  \n  # OpenCode.ai Provider\n  - model_name: shin-4.6\n    litellm_params:\n      model: openai/big-pickle\n      api_key: os.environ/OPENCODE_API_KEY\n      api_base: https://opencode.ai/zen/v1\n  \n  # iFlow Provider - Multiple Models\n  - model_name: otsu-qwen3-coder-plus\n    litellm_params:\n      model: openai/qwen3-coder-plus\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  - model_name: otsu-qwen3-max\n    litellm_params:\n      model: openai/qwen3-max\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  - model_name: otsu-qwen3-coder\n    litellm_params:\n      model: openai/qwen3-coder\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  - model_name: otsu-kimi-k2-0905\n    litellm_params:\n      model: openai/kimi-k2-0905\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  - model_name: otsu-glm-4.6\n    litellm_params:\n      model: openai/glm-4.6\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  - model_name: otsu-deepseek-v3.2\n    litellm_params:\n      model: openai/deepseek-v3.2\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  - model_name: otsu-qwen3-235b-thinking\n    litellm_params:\n      model: openai/qwen3-235b-a22b-thinking-2507\n      api_key: os.environ/IFLOW_API_KEY\n      api_base: https://apis.iflow.cn/v1\n  \n  # Minimax1 Provider via anthropic endpoint\n  - model_name: minimax1-claude\n    litellm_params:\n      model: openai/claude\n      api_key: os.environ/MINIMAX1_API_KEY\n      api_base: https://api.minimax.io/anthropic\n\n  # KAT Provider - Custom endpoint\n  - model_name: shin-otsu-pro\n    litellm_params:\n      model: openai/ep-54wdd8-1762521555111681538\n      api_key: os.environ/KAT_API_KEY\n      api_base: https://vanchin.streamlake.ai/api/gateway/v1/endpoints\n\n  - model_name: shin-otsu-plus\n    litellm_params:\n      model: openai/ep-w4jwca-1760471819232090405\n      api_key: os.environ/KAT_API_KEY\n      api_base: https://vanchin.streamlake.ai/api/gateway/v1/endpoints\n\n  # Novi Provider\n  - model_name: qwen-kat\n    litellm_params:\n      model: openai/kat-coder\n      api_key: sk_1K9xQu19r9Hen-vah5svLqwaqBnlyRKb60UVqx3LEgc\n      api_base: https://api.novita.ai/openai/v1\n\nlitellm_settings:\n  # Enable/disable features\n  drop_params: true\n  set_verbose: true\n  request_timeout: 600\n  \n  # Caching (optional)\n  # cache: true\n  # cache_params:\n  #   type: \"redis\"\n  #   host: \"localhost\"\n  #   port: 6379\n\ngeneral_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY # Authentication key for the proxy\n  #database_url: \"DATABASE_URL='postgresql://neondb_owner:npg_xOAmMe3W1nkU@ep-calm-boat-a187mu3u-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require'\" # Database disabled (optional - requires prisma setup)\n  \n  # Uncomment to enable detailed logging\n  # log_level: \"DEBUG\"\n\nrouter_settings:\n  routing_strategy: simple-shuffle # Options: simple-shuffle, least-busy, usage-based-routing\n  num_retries: 3\n  retry_after: 2\n  timeout: 600\n  \n  # Load balancing (if you have multiple instances of same model)\n  # enable_loadbalancing: true\n\n","numLines":184,"startLine":1,"totalLines":184}}}
{"parentUuid":"6f5d8fad-640d-437a-b88c-377c5618851d","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_2025110820460804993a8dab1d4214","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"Now I'll restart the LiteLLM server with the same configuration as before:\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"3aa3c836-3cdd-4d4d-a1c3-0583807268a4","timestamp":"2025-11-08T12:46:10.706Z"}
{"parentUuid":"3aa3c836-3cdd-4d4d-a1c3-0583807268a4","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_2025110820460804993a8dab1d4214","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_1obw1zga5yk","name":"Bash","input":{"command":"litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug","description":"Start LiteLLM server with config","run_in_background":true}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"790b3aee-6bb3-4a32-849c-7a204242d655","timestamp":"2025-11-08T12:46:10.825Z"}
{"parentUuid":"790b3aee-6bb3-4a32-849c-7a204242d655","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_1obw1zga5yk","type":"tool_result","content":"Command running in background with ID: d380f8","is_error":false}]},"uuid":"bf591e97-16db-429f-97fc-3043609888a5","timestamp":"2025-11-08T12:46:13.669Z","toolUseResult":{"stdout":"","stderr":"","interrupted":false,"isImage":false,"backgroundTaskId":"d380f8"}}
{"parentUuid":"bf591e97-16db-429f-97fc-3043609888a5","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204615f22594ffa2894a2e","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"Let me check the output to make sure the server started correctly:\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":18,"output_tokens":36,"cache_read_input_tokens":17252}},"type":"assistant","uuid":"bbd2c069-98d1-4858-a06b-354d09f26d04","timestamp":"2025-11-08T12:46:30.146Z"}
{"parentUuid":"bbd2c069-98d1-4858-a06b-354d09f26d04","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204615f22594ffa2894a2e","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_tcfk9rckouq","name":"BashOutput","input":{"bash_id":"d380f8"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":18,"output_tokens":36,"cache_read_input_tokens":17252}},"type":"assistant","uuid":"58edec27-2a34-4b5e-b768-b28214d5fc9a","timestamp":"2025-11-08T12:46:30.153Z"}
{"parentUuid":"58edec27-2a34-4b5e-b768-b28214d5fc9a","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_tcfk9rckouq","type":"tool_result","content":"<status>running</status>\n\n<stdout>\n   ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗\n   ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║\n   ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║\n   ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║\n   ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║\n   ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝\n\n\n\u001b[1;37m#------------------------------------------------------------#\u001b[0m\n\u001b[1;37m#                                                            #\u001b[0m\n\u001b[1;37m#              'I don't like how this works...'               #\u001b[0m\n\u001b[1;37m#        https://github.com/BerriAI/litellm/issues/new        #\u001b[0m\n\u001b[1;37m#                                                            #\u001b[0m\n\u001b[1;37m#------------------------------------------------------------#\u001b[0m\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n\n\n\u001b[32mLiteLLM: Proxy initialized with Config, Set models:\u001b[0m\n\u001b[32m    shin-llama-3-1-nemotron-70b\u001b[0m\n\u001b[32m    shin-llama-3-1-405b\u001b[0m\n\u001b[32m    shin-llama-3-1-70b\u001b[0m\n\u001b[32m    shin-llama-3-1-8b\u001b[0m\n\u001b[32m    shin-mistral-nemo-12b\u001b[0m\n\u001b[32m    shin-mistral-large-2\u001b[0m\n\u001b[32m    shin-moonshotai-kimi-k2\u001b[0m\n\u001b[32m    shin-qwen-coder-480b\u001b[0m\n\u001b[32m    shin-minimax-m2\u001b[0m\n\u001b[32m    shin-4.6\u001b[0m\n\u001b[32m    otsu-qwen3-coder-plus\u001b[0m\n\u001b[32m    otsu-qwen3-max\u001b[0m\n\u001b[32m    otsu-qwen3-coder\u001b[0m\n\u001b[32m    otsu-kimi-k2-0905\u001b[0m\n\u001b[32m    otsu-glm-4.6\u001b[0m\n\u001b[32m    otsu-deepseek-v3.2\u001b[0m\n\u001b[32m    otsu-qwen3-235b-thinking\u001b[0m\n\u001b[32m    minimax1-claude\u001b[0m\n\u001b[32m    shin-otsu-pro\u001b[0m\n\u001b[32m    shin-otsu-plus\u001b[0m\n\u001b[32m    qwen-kat\u001b[0m\n</stdout>\n\n<stderr>\nINFO:     Started server process [36387]\nINFO:     Waiting for application startup.\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:628 - litellm.proxy.proxy_server.py::startup() - CHECKING PREMIUM USER - False\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: litellm_license.py:102 - litellm.proxy.auth.litellm_license.py::is_premium() - ENTERING 'IS_PREMIUM' - LiteLLM License=None\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: litellm_license.py:111 - litellm.proxy.auth.litellm_license.py::is_premium() - Updated 'self.license_str' - None\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:641 - worker_config: {\"model\": null, \"alias\": null, \"api_base\": null, \"api_version\": \"2024-07-01-preview\", \"debug\": false, \"detailed_debug\": true, \"temperature\": null, \"max_tokens\": null, \"request_timeout\": null, \"max_budget\": null, \"telemetry\": true, \"drop_params\": false, \"add_function_to_prompt\": false, \"headers\": null, \"save\": false, \"config\": \"config.yaml\", \"use_queue\": false}\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2271 - \u001b[94m setting litellm.drop_params=True\u001b[0m\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2271 - \u001b[94m setting litellm.set_verbose=True\u001b[0m\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2271 - \u001b[94m setting litellm.request_timeout=600\u001b[0m\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2589 - _alerting_callbacks: {'master_key': 'sk-wkSsJJHrYWxg0dIEyid7jL5gFfejusBlVESnILg4BFD8HqDN'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'combined_model_name': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'stripped_model_name': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'combined_stripped_model_name': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2 in litellm.model_cost: 1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'nvidia/llama-3.1-nemotron-70b-instruct', 'combined_model_name': 'nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct', 'stripped_model_name': 'nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct', 'combined_stripped_model_name': 'nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct in litellm.model_cost: nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'combined_model_name': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'stripped_model_name': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'combined_stripped_model_name': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8 in litellm.model_cost: 4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta/llama-3.1-405b-instruct', 'combined_model_name': 'nvidia_nim/meta/llama-3.1-405b-instruct', 'stripped_model_name': 'nvidia_nim/meta/llama-3.1-405b-instruct', 'combined_stripped_model_name': 'nvidia_nim/meta/llama-3.1-405b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/meta/llama-3.1-405b-instruct in litellm.model_cost: nvidia_nim/meta/llama-3.1-405b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'combined_model_name': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'stripped_model_name': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'combined_stripped_model_name': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e in litellm.model_cost: b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta/llama-3.1-70b-instruct', 'combined_model_name': 'nvidia_nim/meta/llama-3.1-70b-instruct', 'stripped_model_name': 'nvidia_nim/meta/llama-3.1-70b-instruct', 'combined_stripped_model_name': 'nvidia_nim/meta/llama-3.1-70b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/meta/llama-3.1-70b-instruct in litellm.model_cost: nvidia_nim/meta/llama-3.1-70b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'combined_model_name': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'stripped_model_name': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'combined_stripped_model_name': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18 in litellm.model_cost: 4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta/llama-3.1-8b-instruct', 'combined_model_name': 'nvidia_nim/meta/llama-3.1-8b-instruct', 'stripped_model_name': 'nvidia_nim/meta/llama-3.1-8b-instruct', 'combined_stripped_model_name': 'nvidia_nim/meta/llama-3.1-8b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/meta/llama-3.1-8b-instruct in litellm.model_cost: nvidia_nim/meta/llama-3.1-8b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'combined_model_name': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'stripped_model_name': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'combined_stripped_model_name': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8 in litellm.model_cost: 40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'mistralai/mistral-nemo-12b-instruct', 'combined_model_name': 'nvidia_nim/mistralai/mistral-nemo-12b-instruct', 'stripped_model_name': 'nvidia_nim/mistralai/mistral-nemo-12b-instruct', 'combined_stripped_model_name': 'nvidia_nim/mistralai/mistral-nemo-12b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/mistralai/mistral-nemo-12b-instruct in litellm.model_cost: nvidia_nim/mistralai/mistral-nemo-12b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'combined_model_name': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'stripped_model_name': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'combined_stripped_model_name': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3 in litellm.model_cost: b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'mistralai/mistral-large-2-instruct', 'combined_model_name': 'nvidia_nim/mistralai/mistral-large-2-instruct', 'stripped_model_name': 'nvidia_nim/mistralai/mistral-large-2-instruct', 'combined_stripped_model_name': 'nvidia_nim/mistralai/mistral-large-2-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/mistralai/mistral-large-2-instruct in litellm.model_cost: nvidia_nim/mistralai/mistral-large-2-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'combined_model_name': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'stripped_model_name': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'combined_stripped_model_name': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1 in litellm.model_cost: 4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'moonshotai/kimi-k2-instruct-0905', 'combined_model_name': 'nvidia_nim/moonshotai/kimi-k2-instruct-0905', 'stripped_model_name': 'nvidia_nim/moonshotai/kimi-k2-instruct-0905', 'combined_stripped_model_name': 'nvidia_nim/moonshotai/kimi-k2-instruct-0905', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/moonshotai/kimi-k2-instruct-0905 in litellm.model_cost: nvidia_nim/moonshotai/kimi-k2-instruct-0905\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'combined_model_name': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'stripped_model_name': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'combined_stripped_model_name': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df in litellm.model_cost: 32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen/qwen3-coder-480b-a35b-instruct', 'combined_model_name': 'nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct', 'stripped_model_name': 'nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct', 'combined_stripped_model_name': 'nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct in litellm.model_cost: nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'combined_model_name': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'stripped_model_name': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'combined_stripped_model_name': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d in litellm.model_cost: 2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'minimaxai/minimax-m2', 'combined_model_name': 'nvidia_nim/minimaxai/minimax-m2', 'stripped_model_name': 'nvidia_nim/minimaxai/minimax-m2', 'combined_stripped_model_name': 'nvidia_nim/minimaxai/minimax-m2', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/minimaxai/minimax-m2 in litellm.model_cost: nvidia_nim/minimaxai/minimax-m2\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'combined_model_name': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'stripped_model_name': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'combined_stripped_model_name': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8 in litellm.model_cost: 56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'big-pickle', 'combined_model_name': 'openai/big-pickle', 'stripped_model_name': 'openai/big-pickle', 'combined_stripped_model_name': 'openai/big-pickle', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/big-pickle in litellm.model_cost: openai/big-pickle\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'combined_model_name': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'stripped_model_name': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'combined_stripped_model_name': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502 in litellm.model_cost: cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-coder-plus', 'combined_model_name': 'openai/qwen3-coder-plus', 'stripped_model_name': 'openai/qwen3-coder-plus', 'combined_stripped_model_name': 'openai/qwen3-coder-plus', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-coder-plus in litellm.model_cost: openai/qwen3-coder-plus\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'combined_model_name': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'stripped_model_name': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'combined_stripped_model_name': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91 in litellm.model_cost: e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-max', 'combined_model_name': 'openai/qwen3-max', 'stripped_model_name': 'openai/qwen3-max', 'combined_stripped_model_name': 'openai/qwen3-max', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-max in litellm.model_cost: openai/qwen3-max\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'combined_model_name': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'stripped_model_name': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'combined_stripped_model_name': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522 in litellm.model_cost: 93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-coder', 'combined_model_name': 'openai/qwen3-coder', 'stripped_model_name': 'openai/qwen3-coder', 'combined_stripped_model_name': 'openai/qwen3-coder', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-coder in litellm.model_cost: openai/qwen3-coder\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'combined_model_name': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'stripped_model_name': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'combined_stripped_model_name': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156 in litellm.model_cost: ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'kimi-k2-0905', 'combined_model_name': 'openai/kimi-k2-0905', 'stripped_model_name': 'openai/kimi-k2-0905', 'combined_stripped_model_name': 'openai/kimi-k2-0905', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/kimi-k2-0905 in litellm.model_cost: openai/kimi-k2-0905\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'combined_model_name': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'stripped_model_name': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'combined_stripped_model_name': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320 in litellm.model_cost: efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'glm-4.6', 'combined_model_name': 'openai/glm-4.6', 'stripped_model_name': 'openai/glm-4.6', 'combined_stripped_model_name': 'openai/glm-4.6', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/glm-4.6 in litellm.model_cost: openai/glm-4.6\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'combined_model_name': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'stripped_model_name': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'combined_stripped_model_name': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed in litellm.model_cost: fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-v3.2', 'combined_model_name': 'openai/deepseek-v3.2', 'stripped_model_name': 'openai/deepseek-v3.2', 'combined_stripped_model_name': 'openai/deepseek-v3.2', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/deepseek-v3.2 in litellm.model_cost: openai/deepseek-v3.2\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'combined_model_name': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'stripped_model_name': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'combined_stripped_model_name': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8 in litellm.model_cost: c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-235b-a22b-thinking-2507', 'combined_model_name': 'openai/qwen3-235b-a22b-thinking-2507', 'stripped_model_name': 'openai/qwen3-235b-a22b-thinking-2507', 'combined_stripped_model_name': 'openai/qwen3-235b-a22b-thinking-2507', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-235b-a22b-thinking-2507 in litellm.model_cost: openai/qwen3-235b-a22b-thinking-2507\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '901f913fd11ebea5b37738f6851c0e2c7b702abbca9507db6cf09ef1d594b8b3', 'combined_model_name': '901f913fd11ebea5b37738f6851c0e2c7b702abbca9507db6cf09ef1d594b8b3', 'stripped_model_name': '901f913fd11ebea5b37738f6851c0e2c7b702abbca9507db6cf09ef1d594b8b3', 'combined_stripped_model_name': '901f913fd1\n\n... [32 lines truncated] ...\n</stderr>\n\n<timestamp>2025-11-08T12:46:30.161Z</timestamp>"}]},"uuid":"891b868e-d1be-43d6-b9a4-b9f26948c627","timestamp":"2025-11-08T12:46:30.161Z","toolUseResult":{"shellId":"d380f8","command":"litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug","status":"running","exitCode":null,"stdout":"   ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗\n   ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║\n   ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║\n   ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║\n   ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║\n   ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝\n\n\n\u001b[1;37m#------------------------------------------------------------#\u001b[0m\n\u001b[1;37m#                                                            #\u001b[0m\n\u001b[1;37m#              'I don't like how this works...'               #\u001b[0m\n\u001b[1;37m#        https://github.com/BerriAI/litellm/issues/new        #\u001b[0m\n\u001b[1;37m#                                                            #\u001b[0m\n\u001b[1;37m#------------------------------------------------------------#\u001b[0m\n\n Thank you for using LiteLLM! - Krrish & Ishaan\n\n\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n\n\n\u001b[32mLiteLLM: Proxy initialized with Config, Set models:\u001b[0m\n\u001b[32m    shin-llama-3-1-nemotron-70b\u001b[0m\n\u001b[32m    shin-llama-3-1-405b\u001b[0m\n\u001b[32m    shin-llama-3-1-70b\u001b[0m\n\u001b[32m    shin-llama-3-1-8b\u001b[0m\n\u001b[32m    shin-mistral-nemo-12b\u001b[0m\n\u001b[32m    shin-mistral-large-2\u001b[0m\n\u001b[32m    shin-moonshotai-kimi-k2\u001b[0m\n\u001b[32m    shin-qwen-coder-480b\u001b[0m\n\u001b[32m    shin-minimax-m2\u001b[0m\n\u001b[32m    shin-4.6\u001b[0m\n\u001b[32m    otsu-qwen3-coder-plus\u001b[0m\n\u001b[32m    otsu-qwen3-max\u001b[0m\n\u001b[32m    otsu-qwen3-coder\u001b[0m\n\u001b[32m    otsu-kimi-k2-0905\u001b[0m\n\u001b[32m    otsu-glm-4.6\u001b[0m\n\u001b[32m    otsu-deepseek-v3.2\u001b[0m\n\u001b[32m    otsu-qwen3-235b-thinking\u001b[0m\n\u001b[32m    minimax1-claude\u001b[0m\n\u001b[32m    shin-otsu-pro\u001b[0m\n\u001b[32m    shin-otsu-plus\u001b[0m\n\u001b[32m    qwen-kat\u001b[0m","stderr":"INFO:     Started server process [36387]\nINFO:     Waiting for application startup.\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:628 - litellm.proxy.proxy_server.py::startup() - CHECKING PREMIUM USER - False\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: litellm_license.py:102 - litellm.proxy.auth.litellm_license.py::is_premium() - ENTERING 'IS_PREMIUM' - LiteLLM License=None\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: litellm_license.py:111 - litellm.proxy.auth.litellm_license.py::is_premium() - Updated 'self.license_str' - None\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:641 - worker_config: {\"model\": null, \"alias\": null, \"api_base\": null, \"api_version\": \"2024-07-01-preview\", \"debug\": false, \"detailed_debug\": true, \"temperature\": null, \"max_tokens\": null, \"request_timeout\": null, \"max_budget\": null, \"telemetry\": true, \"drop_params\": false, \"add_function_to_prompt\": false, \"headers\": null, \"save\": false, \"config\": \"config.yaml\", \"use_queue\": false}\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2271 - \u001b[94m setting litellm.drop_params=True\u001b[0m\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2271 - \u001b[94m setting litellm.set_verbose=True\u001b[0m\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2271 - \u001b[94m setting litellm.request_timeout=600\u001b[0m\n\u001b[92m12:46:18 - LiteLLM Proxy:DEBUG\u001b[0m: proxy_server.py:2589 - _alerting_callbacks: {'master_key': 'sk-wkSsJJHrYWxg0dIEyid7jL5gFfejusBlVESnILg4BFD8HqDN'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'combined_model_name': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'stripped_model_name': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'combined_stripped_model_name': '1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2 in litellm.model_cost: 1127362a054d7392a805784f7e67f9ab7c07abe0b12bbd454261713205dad3a2\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'nvidia/llama-3.1-nemotron-70b-instruct', 'combined_model_name': 'nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct', 'stripped_model_name': 'nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct', 'combined_stripped_model_name': 'nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct in litellm.model_cost: nvidia_nim/nvidia/llama-3.1-nemotron-70b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'combined_model_name': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'stripped_model_name': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'combined_stripped_model_name': '4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8 in litellm.model_cost: 4ade10ff199946d95a06f3002861f92a175383801f457b3281d5db15783245b8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta/llama-3.1-405b-instruct', 'combined_model_name': 'nvidia_nim/meta/llama-3.1-405b-instruct', 'stripped_model_name': 'nvidia_nim/meta/llama-3.1-405b-instruct', 'combined_stripped_model_name': 'nvidia_nim/meta/llama-3.1-405b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/meta/llama-3.1-405b-instruct in litellm.model_cost: nvidia_nim/meta/llama-3.1-405b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'combined_model_name': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'stripped_model_name': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'combined_stripped_model_name': 'b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e in litellm.model_cost: b00017c1be7527782caeff79df50acbca6298cea9f665451479d5447e89afc7e\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta/llama-3.1-70b-instruct', 'combined_model_name': 'nvidia_nim/meta/llama-3.1-70b-instruct', 'stripped_model_name': 'nvidia_nim/meta/llama-3.1-70b-instruct', 'combined_stripped_model_name': 'nvidia_nim/meta/llama-3.1-70b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/meta/llama-3.1-70b-instruct in litellm.model_cost: nvidia_nim/meta/llama-3.1-70b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'combined_model_name': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'stripped_model_name': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'combined_stripped_model_name': '4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18 in litellm.model_cost: 4b550d5e8809f9a1314ed2a916cc34d8499f58036e3449bc9ee7d2c0f449cd18\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta/llama-3.1-8b-instruct', 'combined_model_name': 'nvidia_nim/meta/llama-3.1-8b-instruct', 'stripped_model_name': 'nvidia_nim/meta/llama-3.1-8b-instruct', 'combined_stripped_model_name': 'nvidia_nim/meta/llama-3.1-8b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/meta/llama-3.1-8b-instruct in litellm.model_cost: nvidia_nim/meta/llama-3.1-8b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'combined_model_name': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'stripped_model_name': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'combined_stripped_model_name': '40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8 in litellm.model_cost: 40a883660f924b7c41367767ec72b29641ede3f3b6c78b609e381c48bcfb6dc8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'mistralai/mistral-nemo-12b-instruct', 'combined_model_name': 'nvidia_nim/mistralai/mistral-nemo-12b-instruct', 'stripped_model_name': 'nvidia_nim/mistralai/mistral-nemo-12b-instruct', 'combined_stripped_model_name': 'nvidia_nim/mistralai/mistral-nemo-12b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/mistralai/mistral-nemo-12b-instruct in litellm.model_cost: nvidia_nim/mistralai/mistral-nemo-12b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'combined_model_name': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'stripped_model_name': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'combined_stripped_model_name': 'b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3 in litellm.model_cost: b300cbe4733742e85b21799ff00111644d266bfe084ee32356903c66be362db3\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'mistralai/mistral-large-2-instruct', 'combined_model_name': 'nvidia_nim/mistralai/mistral-large-2-instruct', 'stripped_model_name': 'nvidia_nim/mistralai/mistral-large-2-instruct', 'combined_stripped_model_name': 'nvidia_nim/mistralai/mistral-large-2-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/mistralai/mistral-large-2-instruct in litellm.model_cost: nvidia_nim/mistralai/mistral-large-2-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'combined_model_name': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'stripped_model_name': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'combined_stripped_model_name': '4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1 in litellm.model_cost: 4cd6c125044c9c17ee13a01c221a883c8c17cb075f7e23c59aade88fcf6211e1\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'moonshotai/kimi-k2-instruct-0905', 'combined_model_name': 'nvidia_nim/moonshotai/kimi-k2-instruct-0905', 'stripped_model_name': 'nvidia_nim/moonshotai/kimi-k2-instruct-0905', 'combined_stripped_model_name': 'nvidia_nim/moonshotai/kimi-k2-instruct-0905', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/moonshotai/kimi-k2-instruct-0905 in litellm.model_cost: nvidia_nim/moonshotai/kimi-k2-instruct-0905\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'combined_model_name': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'stripped_model_name': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'combined_stripped_model_name': '32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df in litellm.model_cost: 32a3ba7eebde6ee3e7b742a663dc897a315834cee15e1b39ec68f585e33116df\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen/qwen3-coder-480b-a35b-instruct', 'combined_model_name': 'nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct', 'stripped_model_name': 'nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct', 'combined_stripped_model_name': 'nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct in litellm.model_cost: nvidia_nim/qwen/qwen3-coder-480b-a35b-instruct\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'combined_model_name': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'stripped_model_name': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'combined_stripped_model_name': '2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d in litellm.model_cost: 2c608961367e4fb77b71af39d041113efafbf0922e46df805d18b2823268bf1d\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'minimaxai/minimax-m2', 'combined_model_name': 'nvidia_nim/minimaxai/minimax-m2', 'stripped_model_name': 'nvidia_nim/minimaxai/minimax-m2', 'combined_stripped_model_name': 'nvidia_nim/minimaxai/minimax-m2', 'custom_llm_provider': 'nvidia_nim'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=nvidia_nim/minimaxai/minimax-m2 in litellm.model_cost: nvidia_nim/minimaxai/minimax-m2\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'combined_model_name': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'stripped_model_name': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'combined_stripped_model_name': '56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8 in litellm.model_cost: 56ba18de789d356e84126cea176a7b593d92aa9bcc5d284036b316638a24e3d8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'big-pickle', 'combined_model_name': 'openai/big-pickle', 'stripped_model_name': 'openai/big-pickle', 'combined_stripped_model_name': 'openai/big-pickle', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/big-pickle in litellm.model_cost: openai/big-pickle\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'combined_model_name': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'stripped_model_name': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'combined_stripped_model_name': 'cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502 in litellm.model_cost: cecbd79f127ad9a42032bd57418834409cf7239c57a427ee9ad95f209286a502\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-coder-plus', 'combined_model_name': 'openai/qwen3-coder-plus', 'stripped_model_name': 'openai/qwen3-coder-plus', 'combined_stripped_model_name': 'openai/qwen3-coder-plus', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-coder-plus in litellm.model_cost: openai/qwen3-coder-plus\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'combined_model_name': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'stripped_model_name': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'combined_stripped_model_name': 'e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91 in litellm.model_cost: e9cb41540e741d1a4a90c0de90d5575ff8f4148c37ea7a1728184219aec1ef91\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-max', 'combined_model_name': 'openai/qwen3-max', 'stripped_model_name': 'openai/qwen3-max', 'combined_stripped_model_name': 'openai/qwen3-max', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-max in litellm.model_cost: openai/qwen3-max\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'combined_model_name': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'stripped_model_name': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'combined_stripped_model_name': '93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522 in litellm.model_cost: 93063c592357c77c9e2e9874f5461fa085a7cf89b5a90b352e13e8a32106e522\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-coder', 'combined_model_name': 'openai/qwen3-coder', 'stripped_model_name': 'openai/qwen3-coder', 'combined_stripped_model_name': 'openai/qwen3-coder', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-coder in litellm.model_cost: openai/qwen3-coder\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'combined_model_name': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'stripped_model_name': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'combined_stripped_model_name': 'ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156 in litellm.model_cost: ed2e31be590a3f7d61dddc45a798a2bc6c34b1b77075bb08d323aa4e399ed156\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'kimi-k2-0905', 'combined_model_name': 'openai/kimi-k2-0905', 'stripped_model_name': 'openai/kimi-k2-0905', 'combined_stripped_model_name': 'openai/kimi-k2-0905', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/kimi-k2-0905 in litellm.model_cost: openai/kimi-k2-0905\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'combined_model_name': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'stripped_model_name': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'combined_stripped_model_name': 'efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320 in litellm.model_cost: efccc09181fc23bba716b8433fbb9ef4b80cff0d1f57d9aa45a01e3712187320\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'glm-4.6', 'combined_model_name': 'openai/glm-4.6', 'stripped_model_name': 'openai/glm-4.6', 'combined_stripped_model_name': 'openai/glm-4.6', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/glm-4.6 in litellm.model_cost: openai/glm-4.6\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'combined_model_name': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'stripped_model_name': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'combined_stripped_model_name': 'fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed in litellm.model_cost: fd9b16db28e1e515c1a0378510d0bac32e99ae97ce0187eb4c4b66743b2ca7ed\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'deepseek-v3.2', 'combined_model_name': 'openai/deepseek-v3.2', 'stripped_model_name': 'openai/deepseek-v3.2', 'combined_stripped_model_name': 'openai/deepseek-v3.2', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/deepseek-v3.2 in litellm.model_cost: openai/deepseek-v3.2\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'combined_model_name': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'stripped_model_name': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'combined_stripped_model_name': 'c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8', 'custom_llm_provider': None}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8 in litellm.model_cost: c82417592aa514dc70948d4d3f3eac608e1e4f75f0ac0579ef206d0a94d4add8\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': 'qwen3-235b-a22b-thinking-2507', 'combined_model_name': 'openai/qwen3-235b-a22b-thinking-2507', 'stripped_model_name': 'openai/qwen3-235b-a22b-thinking-2507', 'combined_stripped_model_name': 'openai/qwen3-235b-a22b-thinking-2507', 'custom_llm_provider': 'openai'}\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:5064 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:2349 - added/updated model=openai/qwen3-235b-a22b-thinking-2507 in litellm.model_cost: openai/qwen3-235b-a22b-thinking-2507\n\u001b[92m12:46:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4822 - checking potential_model_names in litellm.model_cost: {'split_model': '901f913fd11ebea5b37738f6851c0e2c7b702abbca9507db6cf09ef1d594b8b3', 'combined_model_name': '901f913fd11ebea5b37738f6851c0e2c7b702abbca9507db6cf09ef1d594b8b3', 'stripped_model_name': '901f913fd11ebea5b37738f6851c0e2c7b702abbca9507db6cf09ef1d594b8b3', 'combined_stripped_model_name': '901f913fd1\n\n... [32 lines truncated] ...","stdoutLines":45,"stderrLines":144,"timestamp":"2025-11-08T12:46:30.161Z"}}
{"parentUuid":"891b868e-d1be-43d6-b9a4-b9f26948c627","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_202511082046338c4aaa18247d4666","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"Perfect! The LiteLLM server has been successfully restarted. Let me verify it's running correctly:\n"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"output_tokens":0}},"type":"assistant","uuid":"e8dad340-7aca-4c7c-95b1-10bbb6f8d20f","timestamp":"2025-11-08T12:46:35.925Z"}
{"parentUuid":"e8dad340-7aca-4c7c-95b1-10bbb6f8d20f","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_202511082046338c4aaa18247d4666","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"tool_use","id":"call_5j87c3ui1on","name":"Bash","input":{"command":"ps aux | grep litellm","description":"Verify LiteLLM server is running"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":13241,"output_tokens":58,"cache_read_input_tokens":17305}},"type":"assistant","uuid":"3890cd55-30a9-4afe-8657-e27c957b3cf4","timestamp":"2025-11-08T12:46:35.986Z"}
{"parentUuid":"3890cd55-30a9-4afe-8657-e27c957b3cf4","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"call_5j87c3ui1on","type":"tool_result","content":"roshans+   35869  0.3  0.0  12268  7808 ?        Ss   12:46   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug' \\< /dev/null && pwd -P >| /tmp/claude-510d-cwd\nroshans+   36387 19.0  1.5 381864 249524 ?       Sl   12:46   0:04 /home/zeus/miniconda3/envs/cloudspace/bin/python3.12 /home/zeus/miniconda3/envs/cloudspace/bin/litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug\nroshans+   36485 17.7  0.0  12268  8064 ?        Ss   12:46   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'ps aux < /dev/null | grep litellm' && pwd -P >| /tmp/claude-ddc4-cwd\nroshans+   37002  0.0  0.0   7084  2176 ?        S    12:46   0:00 grep litellm","is_error":false}]},"uuid":"9143fc57-6bf3-4979-b014-f083f084fffa","timestamp":"2025-11-08T12:46:36.454Z","toolUseResult":{"stdout":"roshans+   35869  0.3  0.0  12268  7808 ?        Ss   12:46   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug' \\< /dev/null && pwd -P >| /tmp/claude-510d-cwd\nroshans+   36387 19.0  1.5 381864 249524 ?       Sl   12:46   0:04 /home/zeus/miniconda3/envs/cloudspace/bin/python3.12 /home/zeus/miniconda3/envs/cloudspace/bin/litellm --config config.yaml --host 0.0.0.0 --port 4000 --detailed_debug\nroshans+   36485 17.7  0.0  12268  8064 ?        Ss   12:46   0:00 bash -c -l source /teamspace/studios/this_studio/.claude/shell-snapshots/snapshot-bash-1762605923989-2mwyz1.sh && eval 'ps aux < /dev/null | grep litellm' && pwd -P >| /tmp/claude-ddc4-cwd\nroshans+   37002  0.0  0.0   7084  2176 ?        S    12:46   0:00 grep litellm","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"9143fc57-6bf3-4979-b014-f083f084fffa","isSidechain":false,"userType":"external","cwd":"/teamspace/studios/this_studio","sessionId":"44944aff-ce38-4ec2-b760-dc1bbb3563ad","version":"2.0.35","gitBranch":"main","message":{"id":"msg_20251108204638cb375590b4784fbf","type":"message","role":"assistant","model":"glm-4.6","content":[{"type":"text","text":"Great! The LiteLLM server has been successfully restarted. The server is now running with:\n- PID 36387\n- Using the same configuration (config.yaml)\n- Host: 0.0.0.0\n- Port: 4000\n- Detailed debug mode enabled\n\nThe server has loaded all 20 configured models including NVIDIA NIM models, OpenCode, iFlow, Minimax1, KAT, and Novi providers."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":468,"output_tokens":98,"cache_read_input_tokens":30603}},"type":"assistant","uuid":"045ad08c-4334-415c-9b7d-2779e8744b06","timestamp":"2025-11-08T12:46:41.258Z"}
