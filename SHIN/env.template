# LiteLLM Gateway Configuration
# Copy this file to .env and fill in your API keys

# Master key for authenticating with the LiteLLM proxy
LITELLM_MASTER_KEY=sk-1234567890abcdef

# OpenAI API Key
OPENAI_API_KEY=sk-your-openai-key-here

# Anthropic API Key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Azure OpenAI (if using)
# AZURE_API_KEY=your-azure-key-here
# AZURE_API_BASE=https://your-resource.openai.azure.com/

# NVIDIA NIM API (NVIDIA Inference Microservices)
NVIDIA_NIM_API_KEY=your-nvidia-api-key
NVIDIA_NIM_API_BASE=https://integrate.api.nvidia.com/v1

# OpenCode.ai Provider
OPENCODE_API_KEY=your-opencode-api-key

# Z.ai Provider (GLM)
ZAI_API_KEY=your-zai-api-key

# iFlow Provider (Multiple models: Qwen, Kimi, GLM, DeepSeek)
IFLOW_API_KEY=your-iflow-api-key

# Minimax1 Provider
MINIMAX1_API_KEY=your-minimax1-api-key

# KAT Provider (Custom endpoint)
KAT_API_KEY=your-kat-api-key

# Other providers (add as needed)
# COHERE_API_KEY=your-cohere-key
# HUGGINGFACE_API_KEY=your-hf-key
# REPLICATE_API_KEY=your-replicate-key

# Gateway settings (optional)
# LITELLM_PORT=4000
# LITELLM_HOST=0.0.0.0

